{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45811d18-61c4-4a43-a7d4-f0043a10b2a5",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting by shrinking the coefficients. It adds a penalty equal to the square of the magnitude of the coefficients to the loss function.\n",
    "\n",
    "Ridge Regression Equation:\n",
    "Loss\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "Loss=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "where:\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the observed values.\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  are the predicted values.\n",
    "𝛽\n",
    "𝑗\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜆\n",
    "λ is the regularization parameter.\n",
    "Differences from Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "OLS Regression minimizes the sum of the squared residuals (errors) without any regularization term:\n",
    "Loss (OLS)\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "Loss (OLS)=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "Ridge Regression includes the penalty term \n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " , which discourages large coefficients and helps to reduce model complexity and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed8de7-fa94-4502-8724-05fabfe50a3d",
   "metadata": {},
   "source": [
    "Q2\n",
    "The assumptions of Ridge Regression are similar to those of OLS regression, with additional considerations for regularization:\n",
    "\n",
    "Linearity: The relationship between the predictors and the response variable is linear.\n",
    "Independence: The observations are independent of each other.\n",
    "Homoscedasticity: The residuals (errors) have constant variance across all levels of the independent variables.\n",
    "Normality: The residuals are normally distributed (this assumption is more critical for inference than for prediction).\n",
    "Multicollinearity: Ridge Regression can handle multicollinearity better than OLS, but it still assumes that predictors are not perfectly collinear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dddcb-e3ee-49e3-9fe0-690c2059c61f",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "\n",
    "The tuning parameter \n",
    "𝜆\n",
    "λ in Ridge Regression controls the amount of regularization applied. It can be selected using techniques such as:\n",
    "\n",
    "Cross-Validation: Split the data into training and validation sets multiple times, train the model with different \n",
    "𝜆\n",
    "λ values, and choose the one that minimizes the validation error.\n",
    "Grid Search: Specify a range of \n",
    "𝜆\n",
    "λ values and use cross-validation to find the optimal value within this range.\n",
    "Random Search: Randomly sample \n",
    "𝜆\n",
    "λ values and use cross-validation to find the best one.\n",
    "Regularization Path: Plot the coefficients as a function of \n",
    "𝜆\n",
    "λ and select the value that balances model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533593f-c8cb-4ed6-a6a3-6c6605e32b91",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "Ridge Regression is not typically used for feature selection because it does not set coefficients exactly to zero. However, it can be used in conjunction with other methods for feature selection:\n",
    "\n",
    "Recursive Feature Elimination (RFE): Use Ridge Regression to rank features based on their importance and iteratively remove the least important ones.\n",
    "Hybrid Methods: Combine Ridge Regression with Lasso Regression (Elastic Net) to benefit from both regularization and feature selection capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6222d-2e51-44f7-992e-8bd2e3b41f34",
   "metadata": {},
   "source": [
    "Q5\n",
    "\n",
    "Ridge Regression performs well in the presence of multicollinearity. It shrinks the coefficients of correlated predictors, distributing the influence more evenly and reducing the variance of the coefficient estimates. This helps to stabilize the model and improve predictive performance compared to OLS, which can produce highly unstable and large coefficient estimates in the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645cf24-f369-40f2-853e-d03a03ac545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
